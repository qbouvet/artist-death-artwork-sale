{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing pipeline for 'meta_amazon_instant_video.json' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changelog : \n",
    "- v2 : removed model, added artist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='royalblue' size=3>\n",
    "<b>\n",
    "\n",
    "Parameters for the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Set which file to open and where\n",
    "datapath = '../DATA/'\n",
    "df_name = \"meta_Books\"\n",
    "\n",
    "\n",
    "## Specify which features we keep from the original table and which features \n",
    "## we add from the API queries\n",
    "features = ['asin', 'categories', 'description', 'title', 'salesRank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns_to_add = ['actors', 'artist', 'authors', 'creators', 'directors']\n",
    "\n",
    "##  Change here which fields will be extracted from the amazon product\n",
    "##\n",
    "def set_df_cell_from_product(amazonProduct, rowNumber, fieldName1, fieldName2) :\n",
    "    if (amazonProduct is not None) : \n",
    "        amazon_products_df.set_value(rowNumber, \"actors\",\n",
    "                                     amazonProduct.actors)\n",
    "        amazon_products_df.set_value(rowNumber, \"directors\",\n",
    "                                     amazonProduct.directors)\n",
    "        amazon_products_df.set_value(rowNumber, \"creators\", \n",
    "                                     amazonProduct.creators)\n",
    "        amazon_products_df.set_value(rowNumber, \"authors\", \n",
    "                                     amazonProduct.authors)\n",
    "        #amazon_products_df.set_value(rowNumber, \"model\",    # The model field is always empty \n",
    "        #                             amazonProduct.model)\n",
    "        amazon_products_df.set_value(rowNumber, \"artist\",    # Added artist field by editing the API\n",
    "                                     amazonProduct.artist)\n",
    "\n",
    "        \n",
    "## Some less important parameters\n",
    "##\n",
    "        \n",
    "# version number to know which file was generated by which version of the notebook\n",
    "version=2\n",
    "# How frequently should we save our data to file\n",
    "data_save_freq = 7000\n",
    "# How many network errors we accept before we give up\n",
    "network_errors_limit = 8\n",
    "# stuff\n",
    "json_name = df_name+'.json.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'amazon'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ce7b783cb5a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Amazon API querying\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mamazon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAmazonAPI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mamazon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAsinNotFound\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'amazon'"
     ]
    }
   ],
   "source": [
    "#essential imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Sleep\n",
    "import time\n",
    "\n",
    "# Strict JSON conversion\n",
    "import json \n",
    "import gzip \n",
    "\n",
    "# Progress display\n",
    "from IPython.display import clear_output\n",
    "import sys\n",
    "\n",
    "# Amazon API querying\n",
    "from amazon.api import AmazonAPI\n",
    "from amazon.api import AmazonProduct\n",
    "from amazon.api import AsinNotFound\n",
    "\n",
    "from urllib.request import HTTPError\n",
    "from socket import gaierror\n",
    "from urllib.request import URLError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='royalblue' size=3>\n",
    "<b>\n",
    "\n",
    "Open metadata file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Load all ASINs we're going to query - use the metadata files\n",
    "## for this, as they contains each ASIN once and only once.\n",
    "##\n",
    "\n",
    "''' This function was provided on the amazon dataset's webpage\n",
    "    It loads a gzipped file directly into a dataframe\n",
    "'''\n",
    "def gz_to_dataframe(datapath, filename):\n",
    "    def parse(path): \n",
    "        g = gzip.open(path, 'rb') \n",
    "        for l in g: \n",
    "            yield eval(l) \n",
    "    def getDF(path): \n",
    "        i = 0 \n",
    "        df = {} \n",
    "        for d in parse(path): \n",
    "            df[i] = d \n",
    "            i += 1 \n",
    "        return pd.DataFrame.from_dict(df, orient='index') \n",
    "    return getDF(datapath+filename)\n",
    "    \n",
    "amazon_products_df = gz_to_dataframe(datapath, json_name)\n",
    "amazon_products_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "amazon_products_df = amazon_products_df[features]\n",
    "amazon_products_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='royalblue' size=3>\n",
    "<b>\n",
    "\n",
    "Prepare for amazon api usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Sign in with amazon API \n",
    "##\n",
    "\n",
    "def get_amazon_interface():\n",
    "    f = open(\"api_creds\")\n",
    "    ar = f.read().split(\"\\n\")\n",
    "    return AmazonAPI(ar[0], ar[1], ar[2])\n",
    "    return ar[0], ar[1], ar[2]\n",
    "\n",
    "amazon = get_amazon_interface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Here, we define some API query helpers\n",
    "##\n",
    "\n",
    "''' Product lookup with API, asin can be a string ('one by one' lookup)\n",
    "    or a list of strings ('bulk lookup').\n",
    "    bulk lookup provides better performance\n",
    "'''\n",
    "def get_prod(asin) : \n",
    "    if not isinstance(asin, str): \n",
    "        acc_str = str(asin[0])\n",
    "        for e in asin : \n",
    "            acc_str += ','+str(e)\n",
    "        return amazon.lookup(ItemId=acc_str)\n",
    "    else :\n",
    "        return amazon.lookup(ItemId=asin)\n",
    "    \n",
    "''' Splits the interval [start-end] into bulks of size bulksize\n",
    "'''    \n",
    "def gen_bulk_index(start, end, bulksize=10, includeEnd=False):\n",
    "    size = end - start + 1\n",
    "    if(includeEnd):\n",
    "        size+=1\n",
    "    if size==1 : \n",
    "       return [[start]]    \n",
    "    bulks = [list(range(start+(i*bulksize), start + (i+1)*bulksize)) for i in range(0, int(size/bulksize))]\n",
    "    if includeEnd : \n",
    "        bulks.append(list(range(bulks[len(bulks)-1][bulksize-1], end+1)))\n",
    "    else : \n",
    "        bulks.append(list(range(bulks[len(bulks)-1][bulksize-1], end)))\n",
    "    return bulks    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Also, we have some functions to save the state of our data structure - in case we need to \n",
    "## to shutdown the computer and restart the query loop at a later time (not used here)\n",
    "##\n",
    "\n",
    "def save_progress(dataframe, nb_rows_processed):\n",
    "    clear_output()\n",
    "    print(\"SAVING PROGRESS - DONT STOP THE CELL\")\n",
    "    sys.stdout.flush()\n",
    "    time.sleep(3)\n",
    "    dataframe.to_csv(datapath+df_name+\"_temp.csv\", index=False, encoding='utf-8')\n",
    "    file = open(datapath+df_name+\"_progress\", \"w\")\n",
    "    file.write(str(nb_rows_processed))\n",
    "    time.sleep(2)\n",
    "    print(\"SAVE COMPLETED - WAIT FOR QUERY LOOP TO RESUME\")    \n",
    "    time.sleep(5)\n",
    "\n",
    "    \n",
    "def load_progress():\n",
    "    dataframe = pd.read_csv(datapath+df_name+\"_temp(v\"+str(version)+\").csv\")\n",
    "    file = open(datapath+df_name+\"_progress\", \"r\")\n",
    "    nb_rows_processed = file.readline()\n",
    "    return dataframe, int(nb_rows_processed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='royalblue' size=3>\n",
    "<b>\n",
    "\n",
    "Bulk lookups parameters and loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Parameters & initialization for bulk item lookup\n",
    "##\n",
    "\n",
    "bulksize = 10\n",
    "\n",
    "# Change this to restore progress from file\n",
    "fresh_run = True\n",
    "\n",
    "if fresh_run : \n",
    "    # used to restart from where we were in case of unexepected network error\n",
    "    lastItemLookedUp = 0\n",
    "    for col in columns_to_add : \n",
    "        amazon_products_df[col] = pd.Series(dtype=str)\n",
    "else : \n",
    "    amazon_products_df, lastItemLookedUp = load_progress()\n",
    "    \n",
    "print(\"last item looked up : \", lastItemLookedUp,  \"  -  time : \",time.strftime(\"%H:%M:%S\"), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Querying loop\n",
    "##\n",
    "\n",
    "ref_for_progress = lastItemLookedUp\n",
    "lastErrorMet=lastItemLookedUp\n",
    "loop_complete = False\n",
    "caught_httperrors=0\n",
    "caught_gaierrors=0\n",
    "errors_counter=0\n",
    "\n",
    "def printprogress() :\n",
    "    clear_output()\n",
    "    print(\"    \",int(100 * (bulk[0]+1) / amazon_products_df.shape[0]), \"% completed (\",bulk[0], \" rows)\", \"  -  time : \",time.strftime(\"%H:%M:%S\"))\n",
    "    print(\"         Last Item Looked up : \", lastItemLookedUp, \" / \", amazon_products_df.shape[0])\n",
    "    print(\"         HTTPErrors : \", caught_httperrors)\n",
    "    print(\"         gaierrors : \", caught_gaierrors)\n",
    "    print(\"\\n\\n\\n\")\n",
    "\n",
    "while not loop_complete : \n",
    "    try : \n",
    "        \n",
    "        for bulk in gen_bulk_index(ref_for_progress, amazon_products_df.shape[0], bulksize=bulksize) : \n",
    "            # update progess every 100 items\n",
    "            if ((bulk[0]-(ref_for_progress)) % 50 == 0) : \n",
    "                printprogress()\n",
    "\n",
    "            # get asins for the bulk and fetch the matching AmazonProducts\n",
    "            asins = amazon_products_df['asin'][bulk].tolist()\n",
    "            noAsinFound = False\n",
    "            try : \n",
    "                prods = get_prod(asins)\n",
    "            except AsinNotFound : \n",
    "                noAsinFound = True\n",
    "                \n",
    "            # if query was successful, reset error counter\n",
    "            errors_counter = 0;\n",
    "            \n",
    "            # Then, process each product to add necessary informations in the dataframe\n",
    "            if noAsinFound : \n",
    "                # Skip this bulk and reset the flag\n",
    "                print(\"No Asin Found for bulk : \", bulk)\n",
    "            elif (type(prods) is list) and (len(prods) == bulksize) :              \n",
    "                # Case : we found exactly one result per ASIN\n",
    "                #        process by bulk\n",
    "                    for i, prod in enumerate(prods) : \n",
    "                        set_df_cell_from_product(prod, bulk[i], \"actors\", \"directors\")\n",
    "            elif (type(prods) is list) or (type(prods) is AmazonProduct) :  \n",
    "                # Case : we obtained a list of AmazonProducts or a single AmazonProduct\n",
    "                #        fallback to 1-by-1 querying\n",
    "                for n in bulk :               \n",
    "                    asin = amazon_products_df['asin'][n]\n",
    "                    try : \n",
    "                        prod = get_prod(asin)\n",
    "                    except(AsinNotFound): \n",
    "                        prod = None\n",
    "                    set_df_cell_from_product(prod, n, \"actors\", \"directors\")\n",
    "                    time.sleep(0.5)\n",
    "\n",
    "            # Save progress\n",
    "            lastItemLookedUp = bulk[len(bulk)-1]\n",
    "            \n",
    "            # Save data to file according to specified frequency\n",
    "            if ((bulk[0]-(ref_for_progress)) % data_save_freq == 0) and (bulk[0]-(ref_for_progress))!=0 : \n",
    "                save_progress(amazon_products_df, lastItemLookedUp)\n",
    "            \n",
    "            # limit query frequency to avoid 503 errors\n",
    "            time.sleep(2)\n",
    "            \n",
    "        loop_complete = True\n",
    "        \n",
    "\n",
    "    except HTTPError :\n",
    "        errors_counter += 1\n",
    "        caught_httperrors += 1\n",
    "        # If we didn't make any progress, something must be wrong\n",
    "        if errors_counter > network_errors_limit : \n",
    "            print(\"HTTPError caught at original_lastItemLookedUp  -  breaking\")\n",
    "            break \n",
    "        # else retry\n",
    "        printprogress()\n",
    "        print(\"      httpError\")\n",
    "        ref_for_progress = lastItemLookedUp\n",
    "        lastErrorMet = lastItemLookedUp\n",
    "        \n",
    "    except(gaierror, URLError): \n",
    "        errors_counter += 1\n",
    "        caught_gaierrors += 1\n",
    "        if errors_counter > network_errors_limit : \n",
    "            print(\"gaierror/urlerror caught too many times  -  breaking\")\n",
    "            break \n",
    "        # else retry\n",
    "        printprogress()\n",
    "        print(\"      GAIError\")\n",
    "        ref_for_progress = lastItemLookedUp\n",
    "        lastErrorMet = lastItemLookedUp\n",
    "        \n",
    "     # recovery time in case of error\n",
    "    print(\"       recovery time\")\n",
    "    time.sleep(4)\n",
    "            \n",
    "        \n",
    "\n",
    "if(loop_complete):\n",
    "    clear_output()    \n",
    "    print(\"amazon query loop completed !\")\n",
    "    # save results\n",
    "    amazon_products_df.to_csv(datapath+df_name+\"_processed(v\"+str(version)+\").csv\")\n",
    "    print('Results saved to file !')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raise NotImplementedError(\"Fence\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
